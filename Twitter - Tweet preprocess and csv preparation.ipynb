{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.4/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import csv                                      #to write csv files\n",
    "import json                                     #to load json data and parse\n",
    "from dateutil.parser import parse               #to parse date time onjects\n",
    "import re                                       #regular expressions to clean data\n",
    "import codecs                                   #not used but has some good encoding options which can be explored\n",
    "import unicodedata                              #not used (code is commented) but has a good way to normalize the data.\n",
    "import html                                     #escape html data\n",
    "import string                                   #string manipulation functions\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer  #for sentiment calulation\n",
    "from nltk import tokenize                       #tokenizer to tokenize the words. Not used.\n",
    "from textblob import TextBlob                   #one more package to calculate sentiment. code commented. not used\n",
    "sid = SentimentIntensityAnalyzer()              #create an instance of sentiment intensity analyzer to be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#list of words with apostrophes, used while cleaning text to replace apostrope words with individual words.\n",
    "#works for plain string texts. might not work for byte or unicode encoded texts.\n",
    "global apos_dict\n",
    "apos_dict = {\n",
    "\"aren't\" : \"are not\",\n",
    "\"can't\" : \"can not\",\n",
    "\"couldn't\" : \"could not\",\n",
    "\"didn't\" : \"did not\",\n",
    "\"doesn't\" : \"does not\",\n",
    "\"don't\" : \"do not\",\n",
    "\"hadn't\" : \"had not\",\n",
    "\"hasn't\" : \"has not\",\n",
    "\"haven't\" : \"have not\",\n",
    "\"he'd\" : \"he would\",\n",
    "\"he'll\" : \"he will\",\n",
    "\"he's\" : \"he is\",\n",
    "\"i'd\" : \"I would\",\n",
    "\"i'll\" : \"I will\",\n",
    "\"i'm\" : \"I am\",\n",
    "\"i'd\" : \"I would\",\n",
    "\"i'll\" : \"I will\",\n",
    "\"i'm\" : \"I am\",\n",
    "\"isn't\" : \"is not\",\n",
    "\"it's\" : \"it is\",\n",
    "\"it'll\":\"it will\",\n",
    "\"i've\" : \"I have\",\n",
    "\"let's\" : \"let us\",\n",
    "\"mightn't\" : \"might not\",\n",
    "\"mustn't\" : \"must not\",\n",
    "\"shan't\" : \"shall not\",\n",
    "\"she'd\" : \"she would\",\n",
    "\"she'll\" : \"she will\",\n",
    "\"she's\" : \"she is\",\n",
    "\"shouldn't\" : \"should not\",\n",
    "\"that's\" : \"that is\",\n",
    "\"there's\" : \"there is\",\n",
    "\"they'd\" : \"they would\",\n",
    "\"they'll\" : \"they will\",\n",
    "\"they're\" : \"they are\",\n",
    "\"they've\" : \"they have\",\n",
    "\"we'd\" : \"we would\",\n",
    "\"we're\" : \"we are\",\n",
    "\"weren't\" : \"were not\",\n",
    "\"we've\" : \"we have\",\n",
    "\"what'll\" : \"what will\",\n",
    "\"what're\" : \"what are\",\n",
    "\"what's\" : \"what is\",\n",
    "\"what've\" : \"what have\",\n",
    "\"where's\" : \"where is\",\n",
    "\"who'd\" : \"who would\",\n",
    "\"who'll\" : \"who will\",\n",
    "\"who're\" : \"who are\",\n",
    "\"who's\" : \"who is\",\n",
    "\"who've\" : \"who have\",\n",
    "\"won't\" : \"will not\",\n",
    "\"wouldn't\" : \"would not\",\n",
    "\"y'all\": \"you all\",\n",
    "\"you'd\" : \"you would\",\n",
    "\"you'll\" : \"you will\",\n",
    "\"you're\" : \"you are\",\n",
    "\"you've\" : \"you have\",\n",
    "\"'re\": \" are\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'll\":\"we will\",\n",
    "\"didn't\": \"did not\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of cities that we are interested in. \n",
    "city_list = set(\n",
    "[\"acworth\"\n",
    ",\"adairsville\"\n",
    ",\"aldora\"\n",
    ",\"alpharetta\"\n",
    ",\"atlanta\"\n",
    ",\"auburn\"\n",
    ",\"austell\"\n",
    ",\"avondale estates\"\n",
    ",\"ball ground\"\n",
    ",\"barnesville\"\n",
    ",\"berkeley lake\"\n",
    ",\"bethlehem\"\n",
    ",\"bowdon\"\n",
    ",\"braswell\"\n",
    ",\"bremen\"\n",
    ",\"brooks\"\n",
    ",\"buchanan\"\n",
    ",\"buford\"\n",
    ",\"canton\"\n",
    ",\"carl\"\n",
    ",\"carrollton\"\n",
    ",\"cartersville\"\n",
    ",\"centralhatchee\"\n",
    ",\"chamblee\"\n",
    ",\"clarkston\"\n",
    ",\"college park\"\n",
    ",\"concord\"\n",
    ",\"conyers\"\n",
    ",\"covington\"\n",
    ",\"cumming\"\n",
    ",\"dacula\"\n",
    ",\"dallas\"\n",
    ",\"dawsonville\"\n",
    ",\"decatur\"\n",
    ",\"doraville\"\n",
    ",\"douglasville\"\n",
    ",\"duluth\"\n",
    ",\"dunwoody\"\n",
    ",\"east point\"\n",
    ",\"emerson\"\n",
    ",\"ephesus\"\n",
    ",\"euharlee\"\n",
    ",\"fairburn\"\n",
    ",\"fayetteville\"\n",
    ",\"flovilla\"\n",
    ",\"forest park\"\n",
    ",\"franklin\"\n",
    ",\"gay\"\n",
    ",\"georgia\"\n",
    ",\"good hope\"\n",
    ",\"grantville\"\n",
    ",\"grayson\"\n",
    ",\"greenville\"\n",
    ",\"griffin\"\n",
    ",\"hampton\"\n",
    ",\"hapeville\"\n",
    ",\"haralson\"\n",
    ",\"hiram\"\n",
    ",\"holly springs\"\n",
    ",\"jackson\"\n",
    ",\"jasper\"\n",
    ",\"jenkinsburg\"\n",
    ",\"johns creek\"\n",
    ",\"jonesboro\"\n",
    ",\"kennesaw\"\n",
    ",\"kingston\"\n",
    ",\"lake city\"\n",
    ",\"lawrenceville\"\n",
    ",\"lilburn\"\n",
    ",\"lithonia\"\n",
    ",\"locust grove\"\n",
    ",\"loganville\"\n",
    ",\"lone oak\"\n",
    ",\"lovejoy\"\n",
    ",\"luthersville\"\n",
    ",\"madison\"\n",
    ",\"manchester\"\n",
    ",\"mansfield\"\n",
    ",\"marietta\"\n",
    ",\"mcdonough\"\n",
    ",\"meansville\"\n",
    ",\"milner\"\n",
    ",\"milton\"\n",
    ",\"molena\"\n",
    ",\"monroe\"\n",
    ",\"monticello\"\n",
    ",\"moreland\"\n",
    ",\"morrow\"\n",
    ",\"mount zion\"\n",
    ",\"mountain park\"\n",
    ",\"nelson\"\n",
    ",\"newborn\"\n",
    ",\"newnan\"\n",
    ",\"norcross\"\n",
    ",\"orchard hill\"\n",
    ",\"oxford\"\n",
    ",\"palmetto\"\n",
    ",\"peachtree city\"\n",
    ",\"pine lake\"\n",
    ",\"porterdale\"\n",
    ",\"powder springs\"\n",
    ",\"riverdale\"\n",
    ",\"roopville\"\n",
    ",\"roswell\"\n",
    ",\"sandy springs\"\n",
    ",\"senoia\"\n",
    ",\"shady dale\"\n",
    ",\"sharpsburg\"\n",
    ",\"smyrna\"\n",
    ",\"snellville\"\n",
    ",\"social circle\"\n",
    ",\"statham\"\n",
    ",\"stockbridge\"\n",
    ",\"stone mountain\"\n",
    ",\"sugar hill\"\n",
    ",\"sunny side\"\n",
    ",\"suwanee\"\n",
    ",\"talking rock\"\n",
    ",\"tallapoosa\"\n",
    ",\"temple\"\n",
    ",\"turin\"\n",
    ",\"tyrone\"\n",
    ",\"union city\"\n",
    ",\"villa rica\"\n",
    ",\"waco\"\n",
    ",\"waleska\"\n",
    ",\"walnut grove\"\n",
    ",\"warm springs\"\n",
    ",\"white\"\n",
    ",\"whitesburg\"\n",
    ",\"williamson\"\n",
    ",\"winder\"\n",
    ",\"woodbury\"\n",
    ",\"woodstock\"\n",
    ",\"woolsey\"\n",
    ",\"zebulon\"\n",
    "])\n",
    "\n",
    "\n",
    "#List of states that we are interested in. \n",
    "state_list = set([\"ga\",\"georgia\"])\n",
    "\n",
    "#cities/states that we want to filter and remove.\n",
    "#Note: Since we use bounding box to get tweets, we end up getting tweets from neighboring cities and states.\n",
    "skip_list = set([\"north carolina\", \"florida\", \"alabama\", \"carolina del norte\", \"carolina del norte\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function which reads the tweet text, looks for apostrophe words and replaces them by individual words and sends the text back\n",
    "def apos_words(text):\n",
    "    sentence=[]\n",
    "    for word in text.split():\n",
    "        try:\n",
    "            sentence.append(apos_dict[word.lower()])\n",
    "        except:\n",
    "            sentence.append(' '.join(word.split(\"'\")))\n",
    "    return ' '.join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweet preprocess \n",
    "def tweet_text_preprocess(text):\n",
    "    #Removing unicode(hex) characters. -> #re.sub(r'\\\\x[0-9A-Fa-f]+',r'', text)\n",
    "    #remove the \"RT @username\" from tweet -> #re.sub(r'@\\w+',r'',re.sub(r'rt @\\w+:',r'',text)\n",
    "    text=re.sub(r'@\\w+',r'',re.sub(r'rt @\\w+:',r'',re.sub(r'\\\\x[0-9A-Fa-f]+',r'', re.sub(r'\\\\n',r'',text)))) #,r'(?:RT @[\\w_:]+) ',r''\n",
    "    #Splits the apostrophe words in text -> text=apos_words(text)\n",
    "    #remove the URLs from tweets -> re.sub('(?:(?:https?|ftp):\\/\\/)?[\\w/\\-?=%.]+\\.[\\w/\\-?=%.]+',r'',text)\n",
    "    text=re.sub('(?:(?:https?|ftp):\\/\\/)?[\\w/\\-?=%]+\\.[\\w/\\-?=%]+',r'',apos_words(text))\n",
    "    #Remove any white space characters \n",
    "    #Unescape html chars in text -> html.unescape(text)\n",
    "    #Replace ### with spaces -> replace(\"#\",\" \")\n",
    "    text = ' '.join(re.findall(\"[\\S]+\", html.unescape(text))).replace(\"#\",\" \").replace(\"\\n\",'').replace(\"\\t\",'').replace(\"\\r\",'').replace(\"\\f\",'').replace(\"\\v\",'')\n",
    "    #####text=' '.join(re.findall(\"[^ \\t\\n\\r\\f\\v]+\", text))\n",
    "    \n",
    "    #the below code separates hex codes for emojis and makes them into a list foe each tweet. \n",
    "    #This can be used later to calculate sentiment\n",
    "    #text=re.sub(r'\\\\x.+[0-9]','', text)\n",
    "    #li = '\\\\'.join(re.findall(r'x[0-9A-F]+',text,re.I))\n",
    "    #print(\"hex codes: \", li)\n",
    "    \n",
    "    return ' '.join(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text:  Yall ever just been around hella girls you aint want \n",
      " ......disgusting\n",
      "After stripping white spaces Yall ever just been around hella girls you aint want ......disgusting\n"
     ]
    }
   ],
   "source": [
    "#stripping white space characters in string text:\n",
    "text='Yall ever just been around hella girls you aint want \\n ......disgusting'\n",
    "print(\"original text: \",text)\n",
    "print(\"After stripping white spaces\", ' '.join(re.findall(\"[\\S]+\", text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text:  Yall ever just been around hella girls you ain\\xe2\\x80\\x99t want......disgusting\n",
      "1  Yall ever just been around hella girls you aint want......disgusting\n",
      "2  Yall ever just been around hella girls you aint want......disgusting\n",
      "3  Yall ever just been around hella girls you aint want......disgusting\n",
      "4  Yall ever just been around hella girls you aint want......disgusting\n",
      "Yall ever just been around hella girls you aint want......disgusting\n"
     ]
    }
   ],
   "source": [
    "#Stripping white space characters in byte encoded text:\n",
    "t=b'Yall ever just been around hella girls you ain\\xe2\\x80\\x99t want......disgusting'\n",
    "s=str(t)[2:len(str(t))-1]\n",
    "print(\"original text: \",s)\n",
    "cleaned_text=tweet_text_preprocess(s)\n",
    "print(' '.join(cleaned_text.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if sid.polarity_scores(\"happy\")['pos'] == 1:\n",
    "#    print(\"pos\")\n",
    "#\n",
    "#sid.polarity_scores(\"bad\")['neu']\n",
    "#sid.polarity_scores(\"bad\")['neg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function which gets the city name based on the place type mentioned in each tweet. Examine a sample json file for clarity.\n",
    "def get_city_info(place_type, name, full_name):\n",
    "    skip_record = other_loc = False\n",
    "    if place_type == \"admin\":\n",
    "        city = name.strip()\n",
    "        state = name.strip()\n",
    "    elif place_type == \"city\":\n",
    "        city = name.strip()\n",
    "        if full_name.lower().split(\",\")[-1].strip() == \"ga\":\n",
    "            state = \"Georgia\"\n",
    "        else:\n",
    "            state = full_name.split(\",\")[-1].strip()\n",
    "    elif place_type == \"poi\":\n",
    "        city = name.strip()\n",
    "        state = \"Georgia\"\n",
    "    elif place_type == \"neighborhood\":\n",
    "        if place_type.lower() in city_list:\n",
    "            city = name.strip()\n",
    "        else:\n",
    "            city = full_name.split(\",\")[-1].strip()\n",
    "            state = \"Georgia\" #if city in city_list else \"NA\"\n",
    "    else:\n",
    "        print (\"A new placetype has been identified: \", place_type)\n",
    "    \n",
    "    #setting the necessary flags\n",
    "    if city.lower() in skip_list:  skip_record = True\n",
    "    if city.lower() not in city_list: other_loc = True\n",
    "    \n",
    "    return(city,state,skip_record,other_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "header = ['Data Source', 'ID', 'User_Id', 'Screen_Name', 'User_Name', 'Original Source', 'Language',\n",
    "           'Time', 'Date', 'Time_Zone', 'Location', 'City', 'State', 'Country', 'Share_Count', 'Favorite_Count',\n",
    "           'Comment_Count', 'URL', 'Description', 'Headlines', 'Encoded_Text', 'Cleaned_Text', 'Sentiment', 'Pos_count','Neg_count','Hashtags']\n",
    "\n",
    "#check whether the file is empty: If it is emoty, write headers.\n",
    "#writing data with location information and without location information into two separate files:\n",
    "with open('non_loc.csv','w',newline='') as fileio:\n",
    "    if fileio.seek(0) in (None, \"\", 0):\n",
    "        writer = csv.writer(fileio)\n",
    "        writer.writerows([header])            \n",
    "    else:\n",
    "        pass\n",
    "with open('loc.csv','w',newline='') as fileio:\n",
    "    if fileio.seek(0) in (None, \"\", 0):\n",
    "        writer = csv.writer(fileio)\n",
    "        writer.writerows([header])            \n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input records                    :  479671\n",
      "Written loc records              :  247892\n",
      "Written non-loc records          :  21123\n",
      "Skipped unwanted-loc records     :  210656\n",
      "Records without coords           :  436002\n",
      "Records with error location      :  2\n",
      "Tweets without URLs              :  0\n",
      "Write errors                     :  0\n",
      "Total written records            :  269015\n"
     ]
    }
   ],
   "source": [
    "#initiate the required counters: \n",
    "error_records=written_loc_records=written_nonloc_records=no_coords=errors=url_error=errors=input_recs=unwanted_location=plc_error=0\n",
    "\n",
    "#open two files as output and the json tweet file as input:\n",
    "with open('twitter_data_location_combined.json','r') as inp, open('non_loc.csv','a',newline='',encoding='utf-8') as op1, open('loc.csv','a',newline='',encoding='utf-8') as op2:\n",
    "    csvWriter1 = csv.writer(op1)\n",
    "    csvWriter2 = csv.writer(op2)\n",
    "    \n",
    "    #Define a translator which is used to remove punctuations. Refer documentation to understand how translator works.\n",
    "    translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "    \n",
    "    #process each record in input file:\n",
    "    for row in inp:\n",
    "        input_recs+=1\n",
    "        #Intiate an empty list. This list will be used to write data into csv file.\n",
    "        tweet_content = []\n",
    "        json_data = json.loads(row,encoding='utf-8')\n",
    "        \n",
    "        #try to get the geo coordinates (if available). Pupolate 'NA' if coordinates are not available\n",
    "        try:\n",
    "            coordinates=json.dumps(json_data[\"geo\"][\"coordinates\"]).strip(\"[]\")\n",
    "        except:\n",
    "            coordinates=\"NA\"\n",
    "            no_coords+=1\n",
    "        \n",
    "        #append the tweet data to list to prepare the output record.\n",
    "        #tweet_content=[]\n",
    "        tweet_content.append(\"Twitter\")                               #source\n",
    "        tweet_content.append(json_data[\"id_str\"])                     #Tweet ID\n",
    "        tweet_content.append(json_data[\"user\"][\"id_str\"])             #User ID\n",
    "        tweet_content.append(json_data[\"user\"][\"screen_name\"])        #User screen name\n",
    "        tweet_content.append(json_data[\"user\"][\"name\"])               #User name\n",
    "        \n",
    "        try:\n",
    "            if (len(json_data[\"entities\"][\"urls\"])>0):                #URL of the content of tweet\n",
    "                tweet_original_source = json_data[\"entities\"][\"urls\"][0][\"expanded_url\"]\n",
    "        except:                                                       #If URL is not available, use tweet status link\n",
    "            tweet_original_source = 'https://twitter.com/' + tweet_user_id + '/status/' + tweet_id    \n",
    "            url_error+=1\n",
    "        \n",
    "        tweet_content.append(tweet_original_source)                   #URL source as extracted above\n",
    "        \n",
    "        tweet_content.append(json_data[\"lang\"])                       #Tweet Language\n",
    "        dt = parse(json_data[\"created_at\"])                           #Date and time, parsed by parser from dateutil\n",
    "        tweet_content.append(str(dt.time()))                          #get time from above datetime object\n",
    "        tweet_content.append(str(dt.date()))                          #get date from above datetime object\n",
    "        tweet_content.append(\"UTC\")                                   #UTC as timezone\n",
    "        tweet_content.append(coordinates)                             #Latitude and Longitude coordinates of the tweet\n",
    "        \n",
    "        #process location details\n",
    "        skip_record = other_loc = False\n",
    "        city = state = ''\n",
    "        try:\n",
    "            city,state,skip_record,other_loc = get_city_info(json_data[\"place\"][\"place_type\"],json_data[\"place\"][\"name\"],json_data[\"place\"][\"full_name\"])\n",
    "        except:\n",
    "            plc_error+=1\n",
    "            skip_record = True\n",
    "        #if the location information is not of our interest, skip the record. Otherwise, get other information and write the data\n",
    "        #Improvement: This can be first evaluated and the remaining information can be obtained if city is of our interest.\n",
    "        if not skip_record:\n",
    "            tweet_content.append(city)                                    #City from above try except loop\n",
    "            tweet_content.append(state)                                   #State\n",
    "            \n",
    "            tweet_content.append(json_data[\"place\"][\"country_code\"])      #Country from which tweet was tweeted\n",
    "            \n",
    "            tweet_content.append(json_data[\"retweet_count\"])              #share count\n",
    "            tweet_content.append(json_data[\"favorite_count\"])             #favourite count\n",
    "            \n",
    "            tweet_content.append('NA')                                    #comment count -> this is not available\n",
    "            tweet_content.append(tweet_original_source)                   #URL of the tweet\n",
    "            \n",
    "            #tweet_content.append(json_data[\"user\"][\"description\"])       #User profile description\n",
    "            try:\n",
    "                tweet_content.append(json_data[\"user\"][\"description\"].encode('utf-8'))\n",
    "            except:\n",
    "                tweet_content.append('NA')\n",
    "            tweet_content.append('NA')                                    #Headlines -> not available\n",
    "            \n",
    "            #Get the full text of tweet if available. otherwise, use the available text. Refer process document for more info.\n",
    "            try:\n",
    "                tweet_content.append(json_data[\"extended_tweet\"][\"full_text\"].encode('utf-8'))\n",
    "                tweet_text=str(json_data[\"extended_tweet\"][\"full_text\"].encode('utf-8'))\n",
    "            except:\n",
    "                tweet_content.append(json_data[\"text\"].encode('utf-8'))\n",
    "                tweet_text=str(json_data[\"text\"].encode('utf-8'))\n",
    "            \n",
    "            #tweet is byte encoded. None of the string operations work directly. So, it is converted to string. \n",
    "            #leading \"b'\" is removed. trailing single quote is also removed. \n",
    "            tweet_text=tweet_text[2:len(tweet_text)-1]\n",
    "            \n",
    "            #preprocess the tweet to remove hex characters\n",
    "            cleaned_text=tweet_text_preprocess(tweet_text)\n",
    "            \n",
    "            #remove punctuations and convert everything to lower.\n",
    "            #Improvement: Vader package can consider case of the text and provide sentiment. Need not convert to lower.\n",
    "            #Might as well try keeping the punctuations. \n",
    "            cleaned_text= ' '.join(cleaned_text.translate(translator).split()).lower()\n",
    "            \n",
    "            #Append the cleaned text to the list.\n",
    "            tweet_content.append(cleaned_text)                                               #Cleaned tweet text without punctuations\n",
    "            \n",
    "            #Calculate sentiment using sid initiated in the beginning for Sentiment Intensity Analyzer. \n",
    "            tweet_content.append(round(sid.polarity_scores(cleaned_text)['compound'],1))     #sentiment\n",
    "            \n",
    "            #Positive and negative sentiment words. This was initially used for a visualization suggested. This can be removed.\n",
    "            #Sentiment indicator using nltk\n",
    "            positive_count=negative_count=0\n",
    "            for w in cleaned_text.split(\" \"):\n",
    "                if sid.polarity_scores(w)['pos'] == 1:\n",
    "                    positive_count+=1\n",
    "                elif sid.polarity_scores(w)['neg'] == 1:\n",
    "                    negative_count+=1\n",
    "                else:\n",
    "                    pass                                              \n",
    "            tweet_content.append(positive_count)\n",
    "            tweet_content.append(negative_count)\n",
    "            \n",
    "            \n",
    "            #tweet_content.append(round(TextBlob(cleaned_text).sentiment.polarity,2)) #Sentiment indicator using textblob\n",
    "            #tweet_content.append(unicodedata.normalize('NFC', tweet_text).encode('ascii', 'ignore'))   \n",
    "            #tweet_content.append(json_data[\"text\"].encode('utf-8'))       #Write the original text with hex codes. Shall be used later to extract sentiment.\n",
    "            \n",
    "            #extract hash tags from json file instead of tweet text.\n",
    "            hashs=[]\n",
    "            if len(json_data[\"entities\"][\"hashtags\"])>0:                  #Extract hash tags from tweets if the length of list >0\n",
    "                for dicts in json_data[\"entities\"][\"hashtags\"]:\n",
    "                    hashs.append(dicts[\"text\"].lower())\n",
    "                tweet_content.append(','.join(hashs))\n",
    "            \n",
    "            #writing non location records and location based records to two different files.\n",
    "            if other_loc:\n",
    "                csvWriter1.writerow(tweet_content)\n",
    "                written_nonloc_records+=1\n",
    "            else:\n",
    "                csvWriter2.writerow(tweet_content)\n",
    "                written_loc_records+=1\n",
    "        else:\n",
    "            unwanted_location+=1\n",
    "\n",
    "print(\"Input records                    : \", input_recs)\n",
    "print(\"Written loc records              : \", written_loc_records)\n",
    "print(\"Written non-loc records          : \", written_nonloc_records)\n",
    "print(\"Skipped unwanted-loc records     : \", unwanted_location)\n",
    "print(\"Records without coords           : \", no_coords)\n",
    "print(\"Records with error location      : \", plc_error)\n",
    "print(\"Tweets without URLs              : \", url_error)\n",
    "print(\"Write errors                     : \", error_records)\n",
    "print(\"Total written records            : \", written_loc_records+written_nonloc_records)\n",
    "#print(\"Skipped total records            : \", errors)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
